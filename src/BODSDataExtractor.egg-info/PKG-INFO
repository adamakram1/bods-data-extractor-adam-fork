Metadata-Version: 2.1
Name: BODSDataExtractor
Version: 0.1.81
Summary: Test package that allows users to download analytical ready fares and timetables data from the UK Bus Open Data Service Platform
Author-email: "Ali Partner, Spencer Brittain" <ali.partner@kpmg.co.uk>
Project-URL: Homepage, https://github.com/KPMG-UK/bods_data_extractor
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.7
Description-Content-Type: text/markdown
License-File: LICENCE


<!-- PROJECT LOGO -->
<br />
<div align="center">
  <a href="https://github.com/github_username/repo_name">
    <img src="https://user-images.githubusercontent.com/105863784/190128076-b6630a01-5809-4018-ae11-b7da83ce131c.png" alt="Logo" width="427" height="66">
    
  </a>

<h3 align="center">BODS Data Extractor</h3>

  <p align="center">
    A python client for downloading and extracting data from the UK Bus Open Data Service
    <br />
    <a href="https://github.com/KPMG-UK/bods_pseudo_test"><strong>Explore the docs »</strong></a>
    <br />
    <br />
    <a href="https://github.com/github_username/repo_name">View Demo</a>
    ·
    <a href="https://github.com/KPMG-UK/bods_pseudo_test/issues">Report Bug</a>
    ·
    <a href="https://github.com/KPMG-UK/bods_pseudo_test/issues">Request Feature</a>
  </p>
</div>



<!-- TABLE OF CONTENTS -->
<details>
  <summary>Table of Contents</summary>
  <ol>
    <li>
      <a href="#about-the-project">About The Project</a>
      <ul>
        <li><a href="#built-with">Built With</a></li>
      </ul>
    </li>
    <li>
      <a href="#getting-started">Getting Started</a>
      <ul>
        <li><a href="#prerequisites">Prerequisites</a></li>
        <li><a href="#installation">Installation</a></li>
      </ul>
    </li>
    <li><a href="#usage">Usage</a></li>
    <li><a href="#roadmap">Roadmap</a></li>
    <li><a href="#contributing">Contributing</a></li>
    <li><a href="#license">License</a></li>
    <li><a href="#contact">Contact</a></li>
    <li><a href="#acknowledgments">Acknowledgments</a></li>
  </ol>
</details>



<!-- ABOUT THE PROJECT -->
## About The Project


![image](https://user-images.githubusercontent.com/105863784/190125659-b5dc1d1f-820c-405b-aa9b-5b3b6738dddc.png)




This project was created to lower the barrier to entry for analysis of UK Bus Open Data. It facilitates the fetching and extraction of data, currently focussed on Timetables, into tables to be used for analysis or your own projects.

<p align="right">(<a href="#readme-top">back to top</a>)</p>



### Built With


[![Python][python.com]][python-url]


<p align="right">(<a href="#readme-top">back to top</a>)</p>



<!-- GETTING STARTED -->
## Getting Started

Follow these simple steps to get started with the project. It is recommended that you use a Python IDE rather than a console.

### Prerequisites

Download and install Python 

(If you are unfamiliar with Python, do not worry. This package is designed to be very easy to use, and once you have python installed you can copy and paste example code from below and through only making small tweaks to input parameters get the data or reporting metrics you require.

For those unsure how to install Python, we recommend downloading Anaconda, an open-source distribution for Python, using the following link: 

https://www.anaconda.com/distribution/

Once this is installed, open Anaconda Navigator GUI and ensure 'Spyder' is installed, before launching it. Here you will be able to run code from the examples below, in order to make use of this package's functionalities. 

Please check this is inline with your organisation's policy before installing)

### Installation

1. Get a free API Key at [https://data.bus-data.dft.gov.uk/](https://data.bus-data.dft.gov.uk/)

Make sure to do step 2 in the terminal or command line (if using Anaconda Navigator open Anaconda Prompt and install using the above code here). All other code is to be run in the python IDE of your choice (we recommend Spyder if using Anaconda)
  
2. Install BODS Data Extractor package 
    ```commandline
      pip install --extra-index-url https://test.pypi.org/simple/ BODSDataExtractor
      ```
3. Open up a .py file and save your API to a variable
   ```python
   api = 'ENTER YOUR API KEY'
   ```
4. Follow the steps below in the Usage section. The code examples are ready to copy and paste into your .py file.
   

<p align="right">(<a href="#readme-top">back to top</a>)</p>



<!-- USAGE EXAMPLES -->
## Usage

### Example of timetables output

Timetable data can be extracted at 3 levels (note that these examples are restricted to 1 dataset, of ID 322):

* Dataset level - High level overview of key dataset information (each row represents 1 dataset on BODS platform)

<img width="1408" alt="image" src="https://user-images.githubusercontent.com/81578708/189095249-79a51be0-0145-4712-b538-65d80b76e99a.png">

* Service Line Level - Detailed information about each service and line extracted from individual files within datasets (each row represents 1 timetables xml file)

<img width="1400" alt="image" src="https://user-images.githubusercontent.com/81578708/189094457-ee19dc5b-97b8-409c-a75d-04ca4adb5016.png">

* Stop level - Stop level data in the form of a traditional timetable (1 timetable from each xml file)

<img width="1362" alt="image" src="https://user-images.githubusercontent.com/81578708/189094683-ce047d25-051c-470e-be8e-70e6b5d47eab.png">

### Fundamentals of extracting data using this package

This package is written using object orientated python principles. Put simply, one must initiate an object based upon the subset of BODS timetables data they wish to extract data on, or analyse. Upon intiating an instance of this object, paramaters can be specified to select the desired subset. Once this object instance has been initiated, there is no immediate output in the python console. One can now access attributes of the instance by calling them. 

Note that for the data extraction, all of the processing is done in the initation of the object instance, and not in the accessing of the attributes. This means it make take some time to initiate the object instance, but should take almost no time to access your desired data once the object instance has been initiated.

The examples below should bring bring this to life.

### How to extract Dataset Level data

```python
#intiate an object instance called my_bus_data_object with desired parameters
from BODSDataExtractor.extractor import TimetableExtractor

my_bus_data_object = TimetableExtractor(api_key=api # Your API Key Here
                                 ,limit=1 # How many datasets to view
                                 ,status = 'published' # Only view published datasets
                                 ,service_line_level=False # True if you require Service line data 
                                 ,stop_level=False # True if you require stop level data
                                 )

#save the extracted dataset level data to dataset_level variable
dataset_level = my_bus_data_object.metadata

#export to csv if you wish to save this data
dataset_level.to_csv('dataset_export.csv')
```
### How to extract Service Line Level data
```python
#intiate an object instance called my_bus_data_object with desired parameters
from BODSDataExtractor.extractor import TimetableExtractor

my_bus_data_object = TimetableExtractor(api_key=api # Your API Key Here
                                 ,limit=1 # How many datasets to view
                                 ,status = 'published' # Only view published datasets
                                 ,service_line_level=True # True if you require Service line data 
                                 ,stop_level=False # True if you require stop level data
                                 )

#save the extracted service line level data to service_line_level variable
service_line_level = my_bus_data_object.service_line_extract

#note that in downloading the service line level data, the dataset level will also be downloaded. Can access this as below:
dataset_level = my_bus_data_object.metadata

#export to csv if you wish to save this data
service_line_level.to_csv('service_line_level_export.csv')
```
### How to extract Stop Level data
```python
#intiate an object instance called my_bus_data_object with desired parameters
from BODSDataExtractor.extractor import TimetableExtractor

my_bus_data_object = TimetableExtractor(api_key=api # Your API Key Here
                                 ,limit=1 # How many datasets to view
                                 ,status = 'published' # Only view published datasets
                                 ,service_line_level=True # True if you require Service line data 
                                 ,stop_level=True # True if you require stop level data
                                 )

#save the extracted stop level data to stop_level variable
stop_level = my_bus_data_object.timetable_dict

#note that in downloading stop level the  data, the dataset and service line level will also be downloaded. Can access this as below:
dataset_level = my_bus_data_object.metadata
service_line_level = my_bus_data_object.service_line_extract

#stop_level variable is a dictionary of dataframes, which can be saved to csv as follows
my_bus_data_object.save_timetables_to_csv()

#or can filter to filter timetable results to a specifc licence number of service code
my_bus_data_object.save_filtered_timetables_to_csv('PC0001838:41')

```
### Expected run times and performance

The volume of timetables data available on the BODS platform is very significant, and while this package simplifies the extraction of this data, and processes it into a analytical ready form, the sheer amount of data dictates that it can take a non trivial amount of time to initiate the above object instances. 

One way of getting around this problem is to narrow your requested data request using additional parameters. Another is to run the download once, and save the output to disk as a csv, to allow re loading of this at a later data for reporting analysis. Both of these approaches are outlined in more detail in below sections.

Directly below are some sample expected run times for extracting data using this package. This should still give you a very approximate idea of how long to expect your code to execute, depending on how much data you are trying to extract.

It is important to note that this can vary depending on your local processing power, internet connection and on the nature of the datasets you are extracting (a dataset may contain one xml file, or several hundred).

| Granularity of data extraction    | 1 dataset timing  | 20 dataset timing | 200 datasets timing |
| --------------------------------- | ----------------- | ----------------- | ------------------- |
| Dataset                           | > 0 hrs 1 min     | > 0 hrs 1 min     | 0 hrs 2 min         |
| Service line                      | > 0 hrs 1 min     | > 0 hrs 1 min     | 0 hrs 6 min         |
| Stop                              | 0 hrs 3 min       | > 0 hrs 1 min     | > 0 hrs 1 min       |


### How to fine tune your results using additional parameters

As well as specifying the granularity of data to extract (dataset, service line or stop level), limiting the number of datasets, and retricting to just published datasets, there are a number of addtional parameters that the object instance can be initiated with. These are as follows:

- nocs - _accepts list input of valid National Operator Codes e.g. ['HIPK', 'YCST']_
- search -  _accepts string input of key words to filter for the data set name, data set description, organisation name, or admin name e.g. 'Arriva'_
- bods_compliant - _accepts boolean input (True or False), where True filters for only BODS Compliant datasets. Default value is True_
- atco_code - _accepts list input of the first three characters of ATCO codes (ATCO codes are unique identifiers of UK bus stops, where first three characters signify the admin area they are within). This filters datasets and/or service lines that have stops within the specified admin areas. e.g. ['320','450']_

Example of this:
```python
#intiate an object instance called my_bus_data_object with desired parameters
from BODSDataExtractor.extractor import TimetableExtractor

my_bus_data_object = TimetableExtractor(api_key=api # Your API Key Here
                                 #,limit=1 # commented out limit so will return all results within other parameters
                                 ,status = 'published' 
                                 ,service_line_level=True 
                                 ,stop_level=False 
                                 ,nocs=['FSCE','FGLA','FCYM'] #values must be entered in this list format - each noc within quotes, separated by comma, all within []
                                 ,search='First Bus' # this is actually redundant as nocs are specific to this operator, but included for demo purpose
                                 ,bods_compliant=True
                                 ,atco_code=['320', '450'] # filter to stops within just north and west yorkshire. Values must be entered in this list format - each code within quotes, separated by comma, all within []
                                 )

#save the extracted dataset level data to filtered_dataset_level variable
filtered_dataset_level = my_bus_data_object.metadata

#save the extracted service line level data to dataset_level variable
filtered_service_line_level = my_bus_data_object.service_line_extract

#export to csv if you wish to save this data
filtered_service_line_level.to_csv('filtered_service_line_level_export.csv')
```

### Using Previously Downloaded Data

The previous examples are based on using the same TimetableMetadata object instance created in the first example. This is good for most cases but there may be times when the user wishes to use previously saved exports, without having to extract all the datasets again. In this case, the user can initiate an object, and manually set the `metadata`, `service_line_extract` and `stop_level` attributes. This will then allow the user to run all relevant reporting functions.

```python
from BODSDataExtractor.extractor import TimetableExtractor

service_data_path = "path to your service line level data here"
service_data = pd.read_csv(bods_data_path)

my_bus_data_object = TimetableExtractor(api_key=api  # Your API Key Here
                                     , limit=1)  # set the limit to 1 to avoid waiting for many datasets to be downloaded                                    

my_bus_data_object.full_data_extract = service_data #set the 'full_data_extract' attribute to the service_data variable loaded from the saved csv file

all_sc = my_bus_data_object.count_service_codes() #this function counts all the service codes in a given service line level dataset
```

### Reporting and Analytics

As well as delivering analytical ready data extracts from the BODS platform, this package also contains a variety of functions for analysing the extracted data. Note that each of these functions run on a variable containing the data you have extracted, as outlined earlier in this READme. This allows you to generate custom report metrics on the particular slice of BODS timetables data you are interested in, rather than on all of the data in the platform. The reporting functions, and how to run them, are detailed below.

```python

red_dq = my_bus_data_object.red_dq_scores() #returns the number of operators in a table with red dq scores

less_than_10 = my_bus_data_object.dq_less_than_x(10) # takes an integer as input (in this case 10) and returns a list of operators with a data quality score less than that integer

no_lic_no = my_bus_data_object.no_licence_no() # returns a report listing which datasets contain files which do not have a licence number

services_by_area = services_on_bods_or_otc_by_area(my_bus_data_object) #Generates a dataframe of all service codes published on BODS and/or in the OTC database, indicates whether they are published on both or one of BODS and OTC, and provides the admin area the services has stops within

services_by_area_MI = my_bus_data_object.services_on_bods_or_otc_by_area_mi() #Generates MI from dataframe that lists all service codes from BODS and/or OTC database, by admin area. Specifically notes the number of services from these sources, and the fraction of all within BODS and OTC respectively.

count_of_operators = my_bus_data_object.count_operators() #returns count of distinct operators (measured by operator_name) in a chosen dataset

count_of_service_codes = my_bus_data_object.count_service_codes()# returns count of unique service codes chosen dataset

valid_service_codes = my_bus_data_object.valid_service_codes()# returns count of unique and valid service codes chosen dataset, a dataframe with all the records with valid service codes and a dataframe with all the invalid service codes.

services_published_in_TXC_2_4 = my_bus_data_object.services_published_in_TXC_2_4()#returns percentage of services published in TXC 2.4 schema, and a dataframe of these records, and a dataframe of the records that are not published in this schema

datasets_published_in_TXC_2_4 = my_bus_data_object.datasets_published_in_TXC_2_4()# returns percentage of datasets published in TXC 2.4 schema, and a dataframe of these records, and a dataframe of the records that are not published in this schema

timetables_publishing_mi = my_bus_data_object.timetables_publishing_mi()# returns high level MI for reporting into DFT on progress of publishing of timetables data

percent_published_licences = my_bus_data_object.percent_published_licences(otc) #percentage of registered licences with at least one published service

unpublished_services = my_bus_data_object.registered_not_published_services(otc) #feed in a copy of the BODS timetable data and otc database to return a list of unpublished service codes

published_not_registered_services = my_bus_data_object.published_not_registered_services(otc)#returns a dataframe of services found in the published data from the api (input) which are not found in the otc database (input)
```

Reporting and analytics can be performed with the builtin functions as shown above. This code first fetches the latest copy of the otc database and compares it to a copy of the service line level data extracted using `service_line_extract`. The code returns a dataframe listing information about service codes registered with OTC but not published in the BODS data.


<p align="right">(<a href="#readme-top">back to top</a>)</p>

### OTC Database

The package is also able to pull the latest copy of the OTC Database (Currently England only) from the [gov.uk](https://www.data.gov.uk/dataset/9ea90ed8-de54-4274-92c6-272edd518bfb/traffic-commissioners-local-bus-service-registration) website. The code below demonstrates how this can be done in a single line of code. 
There are two options to downloading the latest copy of the OTC Database using the `otc_db_download` file. The `save_otc_db` function creates a local folder named as the current date and saves a copy of the OTC DB there. 
It also returns a dataframe containing the OTC DB. The second option is to use the `fetch_otc_db` function which only saves the OTC DB to a variable, and does not save it to a folder.

```python
from BODSDataExtractor import otc_db_download

local_otc = otc_db_download.save_otc_db() # download and save a copy of the otc database, as well as assigning it to the 'otc' variable

otc = otc_db_download.fetch_otc_db() #assign a copy of the otc database to the 'otc' variable
```


<!-- ROADMAP & Limitations -->
## Roadmap & Limitations

As the first release of this project, the focus was on getting analytical ready timetable data in the hands of consumers. As a result, there are a number of limitations to bear in mind when using this package. These aim to be addressed in subsequent releases. 

#### Handling non standard files
Whilst all BODS compliant files meet a certain set of standards, there is still variation in exactly how timetables xml files can be populated. This pacakge currently handles files that meet the most common structure, and a number of notable exceptions, however there are some files that stop level timetables will not be generated for. It will, however, generate dataset level and service line level data for all published files. Future releases will aim to close this gap so timetables can be generated for most, if not all files.

#### Handling vehicle journeys that cross midnight
As the PTI standards dictate, sequence number logic is different for services that have stop times which cross midnight. This currently results in the stop level timetable output having vehicle journeys that do not start at sequence number 1. Future releases will include logic that better handles this to provide a more uniform output for such vehicle journeys.

#### Providing additional detail about services
Future releases will extract additional data from timetables xml files in order to provide further detail on services; for example details regarding the days of operation and exceptions around bank holidays, as well as more detailed route information including distance between stops, in addition to time.

#### Incorporating AVL and Fares data
The BODS platform also provides live vehicle location data (AVL), as well as Fares data. Future releases will aim to incorporate functionality for downloading and analysis AVL data initially, and subsequently Fares data once this has been validated.

#### Optimising performance
Generating stop level timetables requries downlaoding and parsing a significant volume of data. It therefore takes quite some time to run the code. Future relases will aim to reduce execution time through optimmisations.

#### And more!
This project has consumers of BODS data at its heart, and so if any other features would be valuable, or any bugs are noticed please get in touch. Details of how to do this can be found in the 'Contributing' section below.

See the [open issues](https://github.com/KPMG-UK/bods_pseudo_test/issues) for a full list of proposed features (and known issues).

<p align="right">(<a href="#readme-top">back to top</a>)</p>



<!-- CONTRIBUTING -->
## Contributing

Contributions are what make the open source community such an amazing place to learn, inspire, and create. Any contributions you make are **greatly appreciated**.

If you have a suggestion that would make this better, please fork the repo and create a pull request. You can also simply open an issue with the tag "enhancement".
Don't forget to give the project a star! Thanks again!

1. Fork the Project
2. Create your Feature Branch (`git checkout -b feature/AmazingFeature`)
3. Commit your Changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the Branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

<p align="right">(<a href="#readme-top">back to top</a>)</p>



<!-- LICENSE -->
## License

Distributed under the General Public License. See `LICENSE.txt` for more information.

<p align="right">(<a href="#readme-top">back to top</a>)</p>






<!-- MARKDOWN LINKS & IMAGES -->
<!-- https://www.markdownguide.org/basic-syntax/#reference-style-links -->
[contributors-shield]: https://img.shields.io/github/contributors/github_username/repo_name.svg?style=for-the-badge
[contributors-url]: https://github.com/KPMG-UK/bods_pseudo_test/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/github_username/repo_name.svg?style=for-the-badge
[forks-url]: https://github.com/github_username/repo_name/network/members
[stars-shield]: https://img.shields.io/github/stars/github_username/repo_name.svg?style=for-the-badge
[stars-url]: https://github.com/github_username/repo_name/stargazers
[issues-shield]: https://img.shields.io/github/issues/github_username/repo_name.svg?style=for-the-badge
[issues-url]: https://github.com/github_username/repo_name/issues
[license-shield]: https://img.shields.io/github/license/github_username/repo_name.svg?style=for-the-badge
[license-url]: https://github.com/github_username/repo_name/blob/master/LICENSE.txt

[linkedin-shield]: https://img.shields.io/badge/-LinkedIn-black.svg?style=for-the-badge&logo=linkedin&colorB=555
[linkedin-url]: https://linkedin.com/in/linkedin_username
[product-screenshot]: images/screenshot.png

[dataset-screenshot]: images/metadata.png
[service-line-screenshot]: images/full_data.png


[python.com]: https://img.shields.io/badge/Python-0769AD?style=for-the-badge&logo=python&logoColor=white
[python-url]: https://www.python.org
